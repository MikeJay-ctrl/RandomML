{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Convnet \n",
    "## 2 convolutional layers with pooling and batchnorm\n",
    "## 2 fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data = MNIST('../data')\n",
    "train_images, train_labels = data.load_training()\n",
    "test_images, test_labels = data.load_testing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training images - 60000\n",
      " Number of training labels - 60000\n",
      " Number of test images -10000\n",
      " Number of test labels - 10000\n"
     ]
    }
   ],
   "source": [
    "## sanity check \n",
    "#check the data was imported correctly as usual\n",
    "\n",
    "print(' Number of training images - {}\\n Number of training labels - {}'.format(len(train_images), len(train_labels)))\n",
    "print(' Number of test images -{}\\n Number of test labels - {}'.format(len(test_images), len(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABpCAYAAAAqXNiiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTPwHh+cztZilzsEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106a2a7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualise the data\n",
    "samples = np.random.randint(60000, size=5)\n",
    "\n",
    "for idx, sample in enumerate(samples):\n",
    "    image = np.asarray(train_images[sample]).reshape(28, 28)\n",
    "    label = train_labels[sample]\n",
    "    \n",
    "    plt.subplot(1, 5, idx+1)\n",
    "    plt.imshow(image,cmap='gray')\n",
    "    plt.title(label)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initially shape of image data was - (60000, 784)\n",
      "after re-shape its now - (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#image data is fine but the arrays are fine but are in the wrong shape... \n",
    "#images are currently in a long row vector need to have spacial information for cnn to be effective \n",
    "\n",
    "print('initially shape of image data was - {}'.format(np.asarray(train_images).shape)) #60000 * 784\n",
    "\n",
    "#first change to NP array\n",
    "train_data = np.array(train_images)\n",
    "test_data  = np.array(test_images)\n",
    "\n",
    "#reshape\n",
    "train_data = train_data.reshape(60000,28, 28)\n",
    "test_data = test_data.reshape(10000,28, 28)\n",
    "\n",
    "print('after re-shape its now - {}'.format(np.asarray(train_data).shape)) #60000 * 28 *28\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABLCAYAAABgOHyfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIEhBl0gkEgfh/wDV044DcizBYgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11166ed30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#sanity check for new data\n",
    "#prove it is correct shape\n",
    "samples = np.random.randint(60000, size = 10)\n",
    "for idx, sample in enumerate(samples):\n",
    "    image = train_data[sample]\n",
    "    label = train_labels[sample]\n",
    "    plt.subplot(1, 10, idx+1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(label)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number - 8\n",
      "is represented by the one-hot vector:  [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "\n",
      "\n",
      "the number - 6\n",
      "is represented by the one-hot vector:  [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "\n",
      "\n",
      "the number - 7\n",
      "is represented by the one-hot vector:  [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#I will also conver the labels into one-hot vectors\n",
    "#such that the position of the number '1' inside the vector indicates its label\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "train_images = np.array(train_images)\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "train_labels_v = np.zeros((60000,10))\n",
    "train_labels_v[np.arange(60000), train_labels] = 1\n",
    "\n",
    "test_labels_v = np.zeros((10000,10))\n",
    "test_labels_v[np.arange(10000), test_labels] = 1\n",
    "\n",
    "##Sanity check:\n",
    "\n",
    "samples = np.random.randint(60000, size=3)\n",
    "\n",
    "for sample in samples:\n",
    "    print('the number - {}'.format(train_labels[sample]))\n",
    "    print('is represented by the one-hot vector: ', train_labels_v[sample])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets train this data!\n",
    "# network structure:\n",
    "# ConvLayer1 -> relu ->  convlayer2 -> relu -> -> FCLayer 1 -> FCLayer 2 -> output\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, input_dims=28, num_filters=(16, 32), filter_sz=5,\n",
    "                 sample_sz=2, num_classes=10, weight_scale=1e-4, initial_params=(1,2)):\n",
    "        \n",
    "        num_l1_filters, num_l2_filters = num_filters\n",
    "        self.stride, self.pad = initial_params\n",
    "        \n",
    "        #initialize the conv filters\n",
    "        self.c1_filters = np.random.normal(scale=weight_scale, size=(num_l1_filters, 1, filter_sz, filter_sz))\n",
    "        self.c1_b = np.zeros([num_l1_filters])\n",
    "        self.c2_filters = np.random.normal(scale=weight_scale, size=(num_l2_filters, 1, filter_sz, filter_sz))\n",
    "        self.c2_b = np.zeros([num_l2_filters])\n",
    "        \n",
    "        #initialize the affine layer weights\n",
    "        self.fc_W1 = np.random.normal(scale=weight_scale, size=(28*28 * 32, 100))\n",
    "        self.fc_b1 = np.zeros([100])\n",
    "        self.fc_W2 = np.random.normal(scale=weight_scale, size=(100, num_classes))\n",
    "        self.fc_b2 = np.zeros([10])\n",
    "        \n",
    "    def convForward(self, x, k, b):\n",
    "        stride, pad = self.stride, self.pad\n",
    "        N = x.shape[0]\n",
    "        H, W = x[0][0].shape[0], x[0][0].shape[1]\n",
    "        F = k.shape[0]\n",
    "        HH, WW = k[0][0].shape[0], k[0][0].shape[1]\n",
    "        \n",
    "        h_out = int(1+ (H - HH + (2 * pad)) / stride )\n",
    "        w_out = int(1+ (W - WW + ((2 * pad))) / stride )\n",
    "        \n",
    "        x_pad = np.pad((x), ( (0,), (0,), (pad,), (pad,)), 'constant')\n",
    "        output = np.zeros((N, F, h_out, w_out))\n",
    "        for n in range(N):\n",
    "            for f in range(F):\n",
    "                for h in range(h_out):\n",
    "                    for w in range(w_out):\n",
    "                        output[n, f, h, w] = np.sum((x_pad[n, :, (h * stride) : ((h+HH)*stride), (w * stride) : ((w+WW)*stride)] * k[f])+ b[f])\n",
    "        \n",
    "        cache = (x, k, b)\n",
    "        return output, cache\n",
    "    \n",
    "    \n",
    "    \n",
    "    def convBackward(self, dout, cache):\n",
    "        \n",
    "        x, k, b = cache\n",
    "        stride, pad = self.stride, self.pad\n",
    "        \n",
    "        N, F, Hh, Ww = dout.shape\n",
    "        N, C, H, W = x.shape\n",
    "        F, Ch, HH, WW = k.shape\n",
    "        print(x.shape)\n",
    "        print(dout.shape)\n",
    "        \n",
    "        db = np.zeros((F))\n",
    "        for f in range(F):\n",
    "            db[f] = np.sum(dout[:,f,:,:])##shape\n",
    "            \n",
    "            \n",
    "        xout = np.pad(x, ((0,), (0,), (pad,), (pad,)), 'constant')\n",
    "        dw = np.zeros((F, C, HH, WW))\n",
    "        \n",
    "        for f in range(F):\n",
    "            for c in range(Ch):\n",
    "                for h in range(HH):\n",
    "                    for w in range(WW):\n",
    "                        sub_x = xout[:, c, h:(h + Hh *stride): stride, w:(w+Ww *stride):stride] #which part of X_pad did you affect\n",
    "                        dw[f, c, h, w] = np.sum(dout[:,f,:,:] * sub_x)\n",
    "                        \n",
    "\n",
    "                    \n",
    "        dx = np.zeros((N, C, H, W))\n",
    "        for n in range(N):\n",
    "            for h in range(H):\n",
    "                for w in range(W):\n",
    "                    for f in range(F):\n",
    "                        for hh in range(Hh):\n",
    "                            for ww in range(Ww):\n",
    "                                mask1 = np.zeros_like(k[f, :, :, :]) #which part of the mask did you mingle with\n",
    "                                mask2 = np.zeros_like(k[f, :, :, :])\n",
    "                                if (h + pad-hh * stride) < HH and (h + pad -hh *stride) >=0:\n",
    "                                      mask1[:, h + pad - hh * stride, :] = 1.0\n",
    "                                if (W + pad - ww * stride) < WW and (W + pad - ww * stride) >= 0:\n",
    "                                    mask2[:, :, W + pad - ww * stride] = 1.0\n",
    "                                w_masked = np.sum(k[f, :, :, :] * mask1 * mask2, axis=(1, 2))\n",
    "                                dx[n, :, h, w] += dout[n, f, hh, ww] * w_masked\n",
    "                                \n",
    "        return {'b':db, 'x':dx, 'w':dw}\n",
    "                    \n",
    "                    \n",
    "    def fc_forward(self, x):\n",
    "        relu = lambda x: np.maximum(0, x)\n",
    "            \n",
    "        a1 = x.dot(self.fc_W1) + self.fc_b1\n",
    "        a1_relu = relu(a1)\n",
    "        scores = a1_relu.dot(self.fc_W2) + self.fc_b2 #logits\n",
    "            \n",
    "        return scores, (x, a1, a1_relu)\n",
    "             \n",
    "        \n",
    "    def fc_backward(self, y, y_cls, scores, cache, reg=0.05):\n",
    "        x, a1, a1_relu = cache\n",
    "        N = y.shape[0]\n",
    "        probabilities = self.softmax(scores)\n",
    "        loss = self.c_e_loss(probabilities, y_cls)\n",
    "            \n",
    "        data_loss = loss\n",
    "        reg_loss = reg * 0.5 * np.sum(self.fc_W1*self.fc_W1) + reg * 0.5 * np.sum(self.fc_W2*self.fc_W2)\n",
    "            \n",
    "        loss = data_loss + reg_loss\n",
    "        dscores = probabilities\n",
    "        dscores[range(N),y_cls] =-1\n",
    "        dscores /= N\n",
    "        \n",
    "\n",
    "        db2 = np.sum(dscores, axis=0)\n",
    "        dW2 = np.dot(a1_relu.T, dscores)\n",
    "            \n",
    "        d_reluIn = a1_relu\n",
    "        d_reluIn[a1 <= 0] = 0\n",
    "        \n",
    "        db1 = np.sum(d_reluIn, axis=0)\n",
    "        dW1 = np.dot(x.T, d_reluIn)\n",
    "        dx  = np.dot(d_reluIn, self.fc_W1.T)                   \n",
    "            \n",
    "        return {\"b1\": db1, \"d2\": db2, \"W1\": dW1, \"W2\": dW2, \"x\": dx}\n",
    "    \n",
    "    \n",
    "    def softmax(self, scores):\n",
    "        scores -= np.max(scores, axis=1, keepdims=True)\n",
    "        e_scores = np.exp(scores)\n",
    "        return e_scores/ np.sum(e_scores, axis=1, keepdims=True) \n",
    "    \n",
    "    def c_e_loss(self, probs, labels):\n",
    "        N = probs.shape[0]\n",
    "        print(labels.shape)\n",
    "        print(\"N\", N)\n",
    "        return (np.sum(-np.log(probs[range(N), labels])) / N)\n",
    "        \n",
    "        \n",
    "    def loss(x, y, y_gt, reg, num_iterations, sample_size, learning_rate):\n",
    "        \n",
    "        num_samples = x.shape[0]\n",
    "        num_epochs = sample_size / num_samples\n",
    "        \n",
    "        for it in range(num_iterations):\n",
    "            batch_idx = np.random.randint(num_samples, size=sample_size)\n",
    "            \n",
    "            y_batch = y[batch_idx]    \n",
    "            y_batch_cls = y_gt[batch_idx]\n",
    "            x_batch = x[batch_idx]\n",
    "            #\n",
    "            # FORWARD PASS\n",
    "            #\n",
    "            # conv layers\n",
    "            c1_out, c1_cache = self.convForward(x_batch, self.c1_filters, self.c1_b)\n",
    "            \n",
    "            c2_out, c2_cache = self.convForward(c1_out, self.c2_filters, self.c2_b)\n",
    "            \n",
    "            # fc layers\n",
    "            # stretch out input - reshape\n",
    "            fc_in_flat = c2_out.reshape(c2_out.shape[0], -1)\n",
    "            \n",
    "            # fc layer\n",
    "            scores, fc_cache = self.fc_forward(fc_in_flat)\n",
    "                 \n",
    "            \n",
    "            #\n",
    "            # BACKWARD PASS\n",
    "            #\n",
    "            fc_grads =  self.fc_backward(y_batch, y_batch_cls, scores, fc_cache)\n",
    "            #\n",
    "            # (UN)Reshape -- change back to shape of kernels\n",
    "            fc_grads['x'] = fc_grads['x'].reshape(c2_out.shape)\n",
    "            \n",
    "            #\n",
    "            #\n",
    "            c2_grads = self.convBackward(fc_grads['x'], c2_cache)\n",
    "            c1_grads = self.convBackward(c2_grads['x'], c1_cache)\n",
    "            \n",
    "            \n",
    "            ##\n",
    "            ## update weights and bias' here\n",
    "            ##\n",
    "            \n",
    "    \n",
    "    def predict(self, x):\n",
    "        c1_out, _ = self.convForward(x, self.c1_filters, self.c1_b)\n",
    "        c2_out, _ = self.convForward(c1_out, self.c2_filters, self.c2_b)\n",
    "        fc_in_flat = c2_out.reshape(c2_out.shape[0], -1)\n",
    "        scores = self.fc_forward(fc_in_flat)\n",
    "        \n",
    "        return np.argmax(scores, axis=1)\n",
    "    \n",
    "    \n",
    "    def backpred(self, x, y, y_cls, sco, cache):\n",
    "\n",
    "        c1_out, c1_cache = self.convForward(x, self.c1_filters, self.c1_b)\n",
    "        c2_out, c2_cache = self.convForward(c1_out, self.c2_filters, self.c2_b)\n",
    "        fc_in_flat = c2_out.reshape(c2_out.shape[0], -1)\n",
    "        print(\"flatening shape: \", fc_in_flat.shape)\n",
    "        probs, cache  = self.fc_forward(fc_in_flat)\n",
    "        \n",
    "        fc_grads =  self.fc_backward(y, y_cls, sco, cache)\n",
    "        print(\"dx, shape: \", fc_grads['x'].shape)\n",
    "        print(\"dw1, shape: \", fc_grads['W1'].shape)\n",
    "        print(\"dw2, shape: \", fc_grads['W2'].shape)\n",
    "        fc_grads['x'] = fc_grads['x'].reshape(c2_out.shape)\n",
    "        c2_grads = self.convBackward(fc_grads['x'], c2_cache)\n",
    "        c1_grads = self.convBackward(c2_grads['x'], c1_cache)\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train model on dataset \n",
    "model = Model(initial_params=(1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 28, 28)\n",
      "(2, 1, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(2, 1, 28, 28)\n",
      "[7 2]\n",
      "[7 2]\n"
     ]
    }
   ],
   "source": [
    "a = test_data[0:2]#[np.newaxis, np.newaxis, :]\n",
    "print(a.shape)\n",
    "a = a[:, np.newaxis,:,:]\n",
    "print(a.shape)\n",
    "print(test_data.shape)\n",
    "print(a.shape)\n",
    "ay = test_labels[0:2]\n",
    "ay_cls = test_labels[0:2].astype(int)\n",
    "print(ay)\n",
    "print(ay_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n"
     ]
    }
   ],
   "source": [
    "#probs, cache = model.predict(a)\n",
    "#print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatening shape:  (2, 25088)\n",
      "(2,)\n",
      "N 2\n",
      "dx, shape:  (2, 25088)\n",
      "dw1, shape:  (25088, 100)\n",
      "dw2, shape:  (100, 10)\n",
      "(2, 16, 28, 28)\n",
      "(2, 32, 28, 28)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-a9cb89ba7d23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mthing1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthing2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0may\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0may_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-193-8c297e4809c5>\u001b[0m in \u001b[0;36mbackpred\u001b[0;34m(self, x, y, y_cls, sco, cache)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dw2, shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mfc_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc2_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mc1_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvBackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-193-8c297e4809c5>\u001b[0m in \u001b[0;36mconvBackward\u001b[0;34m(self, dout, cache)\u001b[0m\n\u001b[1;32m     78\u001b[0m                             \u001b[0;32mfor\u001b[0m \u001b[0mww\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                                 \u001b[0mmask1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#which part of the mask did you mingle with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                                 \u001b[0mmask2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mhh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mHH\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpad\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mhh\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                                       \u001b[0mmask1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpad\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev_tuts/cs231/virtualenvs/cs231_1/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mzeros_like\u001b[0;34m(a, dtype, order, subok)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# needed instead of a 0 to get same result as zeros for for string dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unsafe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "thing1, thing2 = model.backpred(a, ay, ay_cls, probs, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
